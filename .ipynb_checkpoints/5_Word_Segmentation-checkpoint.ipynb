{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Words from Predicted Characters \n",
    "\n",
    "The character extraction pipeline is currently unable to recognize spaces and as a result is unable to differentiate between distinct words. This is a starting point to determine how such an approach would work.\n",
    "\n",
    "Before delving into this, a quick reminder that our pipeline at the moment is : 'Cover Image' -- 'A list of continuous extracted characters' -- 'A continuous string of predcicted characters'.\n",
    "\n",
    "This continous string of all predicted characters is what we feeding into this model with the expected output being a list of predicted words that make up this string.\n",
    "\n",
    "Two things to note:\n",
    "\n",
    "##### 1. We need a more substantial data to better train this model. As of now we are only using data of around 4000 most common words in Italian language. Books in Italian would help the model learn more about frequencies of different words and assign proper weights accordingly\n",
    "\n",
    "##### 2. The model is a little simplistic where we are training from scratch, mostly this was due to us wanting to learn how exactly a word segementation model works. A different approach would be to use pre calculated models and use those for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the data (Input File)\n",
    "\n",
    "We take the input file we using to train our model (for instance a book), tokenize it and store the statistics of word frequency in a counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes the data\n",
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18917144"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = open('./Data/italian_corpora/ita_wikipedia_2010_10K-sentences.txt').read()\n",
    "# text = open('./Data/italian_corpora/Italian_Word_Dictionary.txt').read()\n",
    "text = open('./Data/italian_corpora/ita_news_2005-2009_1M-sentences.txt').read()\n",
    "\n",
    "words = tokens(text)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'francoforte', 'in', 'evidenza', 'il', 'tecnologico', 'infineon', 'seguito', 'dai', 'finanziari']\n"
     ]
    }
   ],
   "source": [
    "# printing some of the tokens, them being at the top has nothing to do \n",
    "# with freqency -- they just show up at the beginning of the document.\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the text to a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('di', 728025),\n",
       " ('e', 476994),\n",
       " ('il', 444696),\n",
       " ('la', 389174),\n",
       " ('che', 355301),\n",
       " ('in', 303721),\n",
       " ('a', 301309),\n",
       " ('per', 276874),\n",
       " ('un', 264513),\n",
       " ('del', 210503)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the most common words, this shows the words with highest\n",
    "# frequency throughout the document\n",
    "Counter(words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Probabilities of Word Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we calculate probabilities of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(counter):\n",
    "    \"Make a probability distribution, given evidence from a Counter.\"\n",
    "    N = sum(counter.values())\n",
    "    return lambda x: counter[x]/N\n",
    "\n",
    "P = pdist(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a sample phrase we check what the individual probabilities of each word are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016931731343801157 cio\n",
      "0.0022417231692056687 questo\n",
      "0.013982713246777632 un\n",
      "0.00028408093737617053 esempio\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('Cio questo Ã¨ un esempio'):\n",
    "    print(P(w), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the above to calculate the probabilities of word sequences, but it would give incorrect results because we would be assuming that each word is being drawn independently of other words.\n",
    "\n",
    "We use the product of individual probabilities to get a better estimate of the probability of a word sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Multiply the numbers together.  (Like `sum`, but with multiplication.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00016931731343801157 cio\n",
      "3.7956254448164883e-07 cio questo\n",
      "5.307314218704175e-09 cio questo un\n",
      "1.50770679819936e-12 cio questo un esempio\n",
      "0.0 cio olaquesto unesempio \n"
     ]
    }
   ],
   "source": [
    "tests = ['cio',\n",
    "         'cio questo',\n",
    "         'cio questo un',\n",
    "         'cio questo un esempio', \n",
    "         'cio olaquesto unesempio ']\n",
    "\n",
    "for test in tests:\n",
    "    print(Pwords(tokens(test)), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the last entry which was a made up sequence of words gives a 0 probability, while the others have low but non-zero ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Segmentation\n",
    "\n",
    "This is the section that is most useful for us. The thinking is that since we cannot detect spaces in the character extraction process, can we take an input of continuous letters with no spaces and recover the sequence of words that make up the input.\n",
    "\n",
    "For e.g. If the page has \"Ugo Foscolo\" and we can only get 'ugofoscolo' at the end of our pipeline, can we write a function split such that \n",
    "\n",
    "split('ugofoscolo')= 'ugo','foscolo'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using memoization to cut down on time by caching results\n",
    "def memo(f):\n",
    "    \"Memoize function f, whose args must all be hashable.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_word_len = max(len(w) for w in counts)\n",
    "max_word_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest word we seen so far is 26, so we put a limit on the possible length for the first word in a split to a few more than the max_word_len obtained. This max_word_len will change depending on which corpus we using to train the model.\n",
    "\n",
    "For example if we splitting 'verylongword' into (split1, split2) -- we put a limit on how long split1 can be since it obviously will be less than or equal to max_word_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(text, start=0, L=max_word_len+4):\n",
    "    \"Return a list of all (first, rest) pairs; start <= len(first) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the outputs obtained from the split function can be seen as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'laspagna'), ('l', 'aspagna'), ('la', 'spagna'), ('las', 'pagna'), ('lasp', 'agna'), ('laspa', 'gna'), ('laspag', 'na'), ('laspagn', 'a'), ('laspagna', '')]\n",
      "[('d', 'rammaticaitaliana'), ('dr', 'ammaticaitaliana'), ('dra', 'mmaticaitaliana'), ('dram', 'maticaitaliana')]\n"
     ]
    }
   ],
   "source": [
    "print(splits('laspagna'))\n",
    "print(splits('drammaticaitaliana', 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the most probable segmentation of text.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        \n",
    "        # we use our previously defined Pwords function to get the most probable\n",
    "        # split amongst all candidate splits\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples Of The Segmentation Function In Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['la', 'spagna']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segment la spagna - Spain\n",
    "segment('laspagna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ugo', 'foscolo']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying on author Ugo Foscolo\n",
    "segment('ugofoscolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'e', 'l', 'la', 'nuova', 'scuola', 'drammatica', 'italiana']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying on 'Della nuova scuola drammatica italiana'\n",
    "segment('Dellanuovascuoladrammaticaitaliana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d',\n",
       " 'e',\n",
       " 'l',\n",
       " 'b',\n",
       " 'a',\n",
       " 'i',\n",
       " 'l',\n",
       " 'a',\n",
       " 'g',\n",
       " 'g',\n",
       " 'i',\n",
       " 'o',\n",
       " 'd',\n",
       " 'i',\n",
       " 'C',\n",
       " 'os',\n",
       " 'tantino',\n",
       " 'poli']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying on del bailaggio di Costantinopoli\n",
    "segment('delbailaggiodiCostantinopoli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the inaccuracy of the results in the last example a little better, we can look at the frequency of the words above in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"del\" -- 210503 occurences\n",
      "\"bailaggio\" -- 0 occurences\n",
      "\"di\" -- 728025 occurences\n",
      "\"Constantinopoli\" -- 0 occurences\n"
     ]
    }
   ],
   "source": [
    "for s in ['del', 'bailaggio', 'di', 'Constantinopoli']:\n",
    "    print('\"{}\" -- {} occurences'.format(s,counts[s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see 2 of the words in that example do not occur at all in the corpus which results in very fragmented and inaccurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_page_x = ['ABONAPARTEDEDICADELYODA', 'UGOHOSCOLO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'bonaparte', 'dedica', 'del', 'yoda']\n",
      "['ugo', 'ho', 'scolo']\n"
     ]
    }
   ],
   "source": [
    "for word in title_page_x:\n",
    "    print(segment(str.lower(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ['A','BONQPQRTE','DEDLCQ','DELLODQ','UGO','FOSCOLO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n",
      "['bon', 'q', 'pq', 'rte']\n",
      "['d', 'ed', 'l', 'cq']\n",
      "['dello', 'dq']\n",
      "['ugo']\n",
      "['foscolo']\n"
     ]
    }
   ],
   "source": [
    "for s1 in f:\n",
    "    print(segment(str.lower(s1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "As we see from the results, we get satisfactory output when dealing with small and simpler phrases. But when trying to segment bigger strings with less commonly occuring words the results are not very useful.\n",
    "\n",
    "This can be offset to a certain extent by training our model on a much larger corpus of Italian Texts. But even then results would be skewed by a very large string which is what our pipeline offers at present.\n",
    "\n",
    "A simple way to improve the results would be to **use the cluster labels that we generated in the character extraction notebook ('3_Character_Extraction') where we clustered the points based on their y coordinates.** \n",
    "\n",
    "Since this effectively translates into characters demarcated into lines of text, we decided to preserve the label for each character through the pipeline and then use those labels to create groups of predicted characters. For instance an image with three lines of text, after prediction would give three different strings (of predicted characters).\n",
    "\n",
    "Each of these strings would be passed through the segmentation function above and the resulting lists would be merged to produce a list of words (in order) that are the final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference : Peter Norvig's [notebook](https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb) on statistical NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
