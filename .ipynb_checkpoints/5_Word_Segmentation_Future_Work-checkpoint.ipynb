{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Words from Predicted Characters \n",
    "\n",
    "The character extraction pipeline is currently unable to recognize spaces and as a result is unable to differentiate between distinct words. This is a starting point to determine how such an approach would work.\n",
    "\n",
    "Before delving into this, a quick reminder that our pipeline at the moment is : 'Cover Image' -- 'A list of continuous extracted characters' -- 'A continuous string of predcicted characters'.\n",
    "\n",
    "This continous string of all predicted characters is what we feeding into this model with the expected output being a list of predicted words that make up this string.\n",
    "\n",
    "Two things to note:\n",
    "\n",
    "##### 1. We need a more substantial data to better train this model. As of now we are only using data of around 4000 most common words in Italian language. Books in Italian would help the model learn more about frequencies of different words and assign proper weights accordingly\n",
    "\n",
    "##### 2. The model is a little simplistic where we are training from scratch, mostly this was due to us wanting to learn how exactly a word segementation model works. A different approach would be to use pre calculated n-gram models and use those for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the data (Input File)\n",
    "\n",
    "We take the input file we using to train our model (for instance a book), tokenize it and store the statistics of word frequency in a counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes the data\n",
    "def tokens(text):\n",
    "    \"List all the word tokens (consecutive letters) in a text. Normalize to lowercase.\"\n",
    "    return re.findall('[a-z]+', text.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7126"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('./nlp/template.txt').read()\n",
    "words = tokens(text)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcune', 'lettere', 'mercantili', 'toscane', 'da', 'colonie', 'genovesi', 'alla', 'fine', 'del']\n"
     ]
    }
   ],
   "source": [
    "# printing some of the tokens, them being at the top has nothing to do \n",
    "# with freqency -- they just show up at the beginning of the document.\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the text to a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('di', 369),\n",
       " ('la', 150),\n",
       " ('de', 140),\n",
       " ('il', 132),\n",
       " ('della', 132),\n",
       " ('e', 132),\n",
       " ('poesie', 122),\n",
       " ('del', 108),\n",
       " ('relazione', 108),\n",
       " ('rime', 95)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the most common words, this shows the words with highest\n",
    "# frequency throughout the document\n",
    "Counter(words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define valid characters, at present we considering only letters\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=counts.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edit functions as explained in the comments return words that perfectly match the input word -- edits0() or are at a distance of 1 or 2 edits -- edits1() and edits2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
    "\n",
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "    \"Correct all the words within a text, returning the corrected text.\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "\n",
    "def correct_match(match):\n",
    "    \"Spell-correct word in match, and preserve proper upper/lower/title case.\"\n",
    "    word = match.group()\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def case_of(text):\n",
    "    \"Return the case-function appropriate for text: upper, lower, title, or just str.\"\n",
    "    return (str.upper if text.isupper() else\n",
    "            str.lower if text.islower() else\n",
    "            str.title if text.istitle() else\n",
    "            str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cio questo è un esempio'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The italian translation for \"This is an example\" is \"Cio questo è un esempio\"\n",
    "# We modify a few letter and see how the correct_text() function works\n",
    "\n",
    "correct_text('Ciago quesyto è uyn gesempio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simple example above, our model was able to auto correct the sentence accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Probabilities of Word Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step would be to calculate probabilities of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(counter):\n",
    "    \"Make a probability distribution, given evidence from a Counter.\"\n",
    "    N = sum(counter.values())\n",
    "    return lambda x: counter[x]/N\n",
    "\n",
    "P = pdist(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004209935447656469 cio\n",
      "0.000140331181588549 questo\n",
      "0.003648610721302273 un\n",
      "0.000140331181588549 esempio\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('Cio questo è un esempio'):\n",
    "    print(P(w), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the above result to calculate the probabilities of word sequences but it would give incorrect results because we would be assuming that each word is being drawn independently of other words.\n",
    "\n",
    "We use the product of individual probabilities to get a better estimate of the probability of a word sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Multiply the numbers together.  (Like `sum`, but with multiplication.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004209935447656469 cio\n",
      "5.907852157811493e-08 cio questo\n",
      "2.155545272285978e-10 cio questo un\n",
      "3.0249021502750184e-14 cio questo un esempio\n",
      "0.0 cio olaquesto unesempio \n"
     ]
    }
   ],
   "source": [
    "tests = ['cio',\n",
    "         'cio questo',\n",
    "         'cio questo un',\n",
    "         'cio questo un esempio', \n",
    "         'cio olaquesto unesempio ']\n",
    "\n",
    "for test in tests:\n",
    "    print(Pwords(tokens(test)), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the last entry which was a made up sequence of words gives a 0 probability, while the others have low but non-zero ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Segmentation\n",
    "\n",
    "This is the section that is most useful for us. The thinking is that since we cannot detect spaces in the character extraction process, can we take an input of continuous letters with no spaces and recover the sequence of words that make up the input.\n",
    "\n",
    "For e.g. If the page has \"Ugo Foscolo\" and we can only get 'ugofoscolo' at the end of our pipeline, can we write a function split such that \n",
    "\n",
    "split('ugofoscolo')= 'ugo','foscolo'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using memoization to cut down on time by caching results\n",
    "def memo(f):\n",
    "    \"Memoize function f, whose args must all be hashable.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_word_len = max(len(w) for w in counts)\n",
    "max_word_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest word we seen so far is 16, so we put a limit on the possible length for the first word in a split to a few more than the max_word_len obtained.\n",
    "\n",
    "For example if we splitting 'verylongword' into (split1, split2) -- we put a limit on how long split1 can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(text, start=0, L=max_word_len+4):\n",
    "    \"Return a list of all (first, rest) pairs; start <= len(first) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the outputs obtained from the split function can be seen as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'laspagna'), ('l', 'aspagna'), ('la', 'spagna'), ('las', 'pagna'), ('lasp', 'agna'), ('laspa', 'gna'), ('laspag', 'na'), ('laspagn', 'a'), ('laspagna', '')]\n",
      "[('d', 'rammaticaitaliana'), ('dr', 'ammaticaitaliana'), ('dra', 'mmaticaitaliana'), ('dram', 'maticaitaliana')]\n"
     ]
    }
   ],
   "source": [
    "print(splits('laspagna'))\n",
    "print(splits('drammaticaitaliana', 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the most probable segmentation of text.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        \n",
    "        # we use our previously defined Pwords function to get the most probable\n",
    "        # split amongst all candidate splits\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['la', 'spagna']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# segment la spagna - Spain\n",
    "segment('laspagna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ugo', 'foscolo']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying on author Ugo Foscolo\n",
    "segment('ugofoscolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'ella', 'nuova', 'scuola', 'drammatica', 'italiana']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying on 'Della nuova scuola drammatica italiana'\n",
    "segment('Dellanuovascuoladrammaticaitaliana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d',\n",
       " 'e',\n",
       " 'l',\n",
       " 'b',\n",
       " 'a',\n",
       " 'i',\n",
       " 'l',\n",
       " 'a',\n",
       " 'g',\n",
       " 'g',\n",
       " 'i',\n",
       " 'o',\n",
       " 'd',\n",
       " 'i',\n",
       " 'C',\n",
       " 'o',\n",
       " 'sta',\n",
       " 'n',\n",
       " 'ti',\n",
       " 'no',\n",
       " 'po',\n",
       " 'li']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying on del bailaggio di Costantinopoli\n",
    "segment('delbailaggiodiCostantinopoli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, trying the same string as above without the word \"Constantinopoli\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['del', 'bailaggio', 'di']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying on del bailaggio di (Same as above without Constantinopoli)\n",
    "segment('delbailaggiodi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple explanation of why we get much better segementation without one word can be seen in the freqency of the constituent words as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"del\" -- 108 occurences\n",
      "\"bailaggio\" -- 1 occurences\n",
      "\"di\" -- 369 occurences\n",
      "\"Constantinopoli\" -- 0 occurences\n"
     ]
    }
   ],
   "source": [
    "for s in ['del', 'bailaggio', 'di', 'Constantinopoli']:\n",
    "    print('\"{}\" -- {} occurences'.format(s,counts[s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "As we see from the results, we get satisfactory output when dealing with small and simpler phrases. But when trying to segment bigger strings with less commonly occuring words the results are not very useful.\n",
    "\n",
    "This can be offset to a certain extent by training our model on a much larger corpus of Italian Texts. But even then results would be skewed by a very large string which is what our pipeline offers at present.\n",
    "\n",
    "A simple way to improve the results would be to **use the cluster labels that we generated in the character extraction notebook ('3_Character_Extraction') where we clustered the points based on their y coordinates.** \n",
    "\n",
    "Since this effectively translates into characters demarcated into lines of text, we decided to preserve the label for each character through the pipeline and then use those labels to create groups of predicted characters. For instance an image with three lines of text, after prediction would give three different strings (of predicted characters).\n",
    "\n",
    "Each of these strings would be passed through the segmentation function above and the resulting lists would be merged to produce a list of words (in order) that are the final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference : Peter Norvig's [notebook](https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb) on statistical NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
