{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our Classification Model\n",
    "\n",
    "This notebook outlines how we utilized both printed and handwritten characters to train our classification model and goes through the different classes of models that were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import os\n",
    "import PIL.Image\n",
    "import random\n",
    "import scipy.ndimage\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "We used two data sources to train our classification model, handwritten letters from the [EMNIST dataset](https://www.nist.gov/itl/iad/image-group/emnist-dataset) and printed letters from the [Chars74K dataset](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/). Though we will ultimately be classifying printed characters from title pages, we've decided to train our model using both printed _and_ handwritten characters in the hopes of making our model more robust. Both data sources consist of 62 classes of characters (uppercase and lowercase for all letters in the English alphabet and all 10 digits), but we will, initially, only be using the letter characters to train our model since title pages consist predominantly of letter characters. We will also be treating uppercase and lowercase versions of the same letter as the same class since our classifications don't need to be case-sensitive and merging these classes improves accuracy.\n",
    "\n",
    "### EMNIST dataset (handwritten)\n",
    "\n",
    "Character images are 28x28 pixels with white text on a black background. The distribution of images per class is unbalanced and is as follows:\n",
    "\n",
    "|Letter|Uppercase|Lowercase|Total|\n",
    "|------|---------|---------|-----|\n",
    "|A|7469|11677|19146|\n",
    "|B|4526|6012|10538|\n",
    "|C|11833|3286|15119|\n",
    "|D|5341|11860|17201|\n",
    "|E|5785|28723|34508|\n",
    "|F|10622|2961|13583|\n",
    "|G|2964|4276|7240|\n",
    "|H|3673|10217|13890|\n",
    "|I|13994|3152|17146|\n",
    "|J|4388|2213|6601|\n",
    "|K|2850|2957|5807|\n",
    "|L|5886|17853|23739|\n",
    "|M|10487|3109|13596|\n",
    "|N|9588|13316|22904|\n",
    "|O|29139|3215|32354|\n",
    "|P|9744|2816|12560|\n",
    "|Q|3018|3499|6517|\n",
    "|R|5882|16425|22307|\n",
    "|S|24272|3136|27408|\n",
    "|T|11396|21227|32623|\n",
    "|U|14604|3312|17916|\n",
    "|V|5433|3378|8811|\n",
    "|W|5501|3164|8665|\n",
    "|X|3203|3292|6495|\n",
    "|Y|5541|2746|8287|\n",
    "|Z|3165|3176|6341|\n",
    "\n",
    "### Chars74K dataset (printed)\n",
    "\n",
    "Character images are 128x128 pixels with black text on a white background. The distribution of images per class is balanced with 1,016 images for each class. The 1,016 images for each class consist of 254 different fonts where each font uses one of 4 styles of emphasis (normal, _italics_, **bold**, or ***bold & italics***). The grayscale of these printed character images is reversed before the data is zipped up for use in this notebook. All other transformations necessary to make the printed character dataset consistent with the handwritten character dataset are contained within this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of data that is available to use in this notebook is 293 MB in size and is available to download [here](https://drive.google.com/file/d/1Lz9bK85vTPnjS_D7WErI_zXIssIVdgro/view?usp=sharing). The data consists of 26 numbered subdirectories (one for each letter with the number representing the index of the letter in the English alphabet) where the file names encode whether the character is printed or handwritten, the case of the letter (printed or handwritten), and for printed characters, an identifier for the font."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Images to Train/Test the Model\n",
    "\n",
    "Below is a function that selects which of the available images to use as training data and test data. The function is helpful with testing different handwritten/printed splits of the training data to see which split produces the \"best\" model. Since the available data consists of nearly 8 times more handwritten characters than printed characters, depending on the specified split between handwritten/printed characters in the training data, we aren't always going to use all of the available handwritten data to train our model. At the moment, these behaviors are hard coded into this process:\n",
    "\n",
    "*  training/test data will use all of the available printed characters\n",
    "*  training/test data will be balanced by letter/case combination, but it might be a better idea to distribute the frequency of letters in the training/test data to more closely match the natural frequency of letters occurring in the Italian language\n",
    "*  fonts of printed characters that appear in the training data will never appear in the test data as well\n",
    "*  test data will be 100% printed characters (since we will ultimately be classifying printed characters from title pages), but if test accuracy is used as one of the main metrics to compare models, it might not be a bad idea to use some handwritten characters in the test set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of emphases used for each font/letter/case combination in the printed dataset\n",
    "# (normal, italics, bold, or bold & italics)\n",
    "CHARS_PER_FONT = 4\n",
    "\n",
    "# the minimum number of images for a letter/case combination in the handwritten dataset\n",
    "# (lowercase J)\n",
    "MIN_HANDWRITTEN_CLASS_SIZE = 2213\n",
    "\n",
    "# the number of fonts used in the printed dataset\n",
    "NUM_UNIQUE_FONTS = 254\n",
    "\n",
    "# the number of images for each letter in the printed dataset (combining the totals of \n",
    "# lowercase and uppercase letters)\n",
    "PRINTED_CHARS_PER_CLASS = 2032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function that uses random components to determine which of the available character\n",
    "images to use as training data and test data.\n",
    "\n",
    "Args:\n",
    "    train_ratio (float): the percentage of the combined training/test data that is used in the\n",
    "        training set\n",
    "    printed_train_ratio (float): the percentage of the training data that are printed\n",
    "        characters (as opposed to handwritten characters)\n",
    "    data_dir (string): path to where the data directory is located\n",
    "\n",
    "Returns:\n",
    "    A 2-tuple of the full paths of images to use in the training set and the full paths of\n",
    "    images to use in the testing set.\n",
    "\"\"\"\n",
    "\n",
    "def get_data(train_ratio, printed_train_ratio, data_dir):\n",
    "\n",
    "    if printed_train_ratio == 1:\n",
    "        # the parameters specify that only printed characters should be used in the training\n",
    "        # set\n",
    "        handwritten_train = 0\n",
    "        printed_train = PRINTED_CHARS_PER_CLASS * train_ratio\n",
    "    else:\n",
    "        # otherwise, estimate the number of handwritten and printed characters to use as \n",
    "        # training data from each class from a system of equations using the specified\n",
    "        # parameters\n",
    "        common_term = train_ratio * (1 - printed_train_ratio)\n",
    "        handwritten_train = (common_term * PRINTED_CHARS_PER_CLASS) / (1 - common_term)\n",
    "        printed_train = (printed_train_ratio * handwritten_train) / (1 - printed_train_ratio)\n",
    "\n",
    "    # round the estimated number of handwritten characters to use as training data up to the\n",
    "    # nearest even number so we are using the same number of uppercase letters as lowercase\n",
    "    # letters\n",
    "    handwritten_train = math.ceil(handwritten_train / 2) * 2\n",
    "    # round the estimated number of printed characters to use as training data down to the\n",
    "    # point where we are using all of the characters of a certain font\n",
    "    printed_train = math.floor(printed_train / (CHARS_PER_FONT * 2)) * (CHARS_PER_FONT * 2)\n",
    "    # after rounding both the number of handwritten and printed characters to use as training\n",
    "    # data, increase the number of handwritten characters to use as training data until we've\n",
    "    # reached the specified ratio of printed characters to handwritten characters in the\n",
    "    # training set\n",
    "    while printed_train / (handwritten_train + printed_train) > printed_train_ratio:\n",
    "        handwritten_train += 2\n",
    "\n",
    "    # the number of handwritten characters to use for each letter/case combination\n",
    "    handwritten_train_per_class = handwritten_train // 2\n",
    "    # the number of printed fonts to use for each letter/case combination\n",
    "    printed_train_fonts_per_class = printed_train // (CHARS_PER_FONT * 2)\n",
    "\n",
    "    # raise an exception if the specified percentage of printed characters in the training set\n",
    "    # is small enough where we're not guaranteed to have a balanced number of handwritten\n",
    "    # characters in the training set by letter/case combination\n",
    "    assert handwritten_train_per_class <= MIN_HANDWRITTEN_CLASS_SIZE, (\n",
    "        \"Not enough available handwritten characters to use a constant number of handwritten \"\n",
    "        \"characters in the training set for each letter/case combination. Please increase \"\n",
    "        \"the percentage of printed characters to use in the training set.\"\n",
    "    )\n",
    "\n",
    "    # will store the full paths of all images to use in the training set and test set,\n",
    "    # respectively; these lists will then be returned by the function\n",
    "    train_images, test_images = [], []\n",
    "\n",
    "    for class_dir in os.listdir(data_dir):\n",
    "\n",
    "        # for each class, will store the full paths of all handwritten uppercase and lowercase\n",
    "        # images belonging to that class, respectively\n",
    "        handwritten_uppercase, handwritten_lowercase = [], []\n",
    "\n",
    "        # for each class, will store the full paths of all printed images that belong to that\n",
    "        # class, partitioned by font\n",
    "        fonts = list(range(NUM_UNIQUE_FONTS))\n",
    "        printed_by_font = {font: [] for font in fonts}\n",
    "\n",
    "        for char_image in os.listdir(os.path.join(data_dir, class_dir)):\n",
    "\n",
    "            # the image file name encodes handwritten/printed, case of letter and identifies\n",
    "            # the font for printed images\n",
    "            filename_parts = char_image.split(\"_\")\n",
    "            char_image_full_path = os.path.join(data_dir, class_dir, char_image)\n",
    "\n",
    "            if filename_parts[1] == \"handwritten\":\n",
    "                if filename_parts[-1][:-4] == \"uppercase\":\n",
    "                    handwritten_uppercase.append(char_image_full_path)\n",
    "                else:\n",
    "                    assert filename_parts[-1][:-4] == \"lowercase\", (\n",
    "                        \"Unexpected format of image file name: {}\".format(char_image)\n",
    "                    )\n",
    "                    handwritten_lowercase.append(char_image_full_path)\n",
    "            else:\n",
    "                assert filename_parts[1] == \"printed\", (\n",
    "                    \"Unexpected format of image file name: {}\".format(char_image)\n",
    "                )\n",
    "                printed_by_font[int(filename_parts[2][-3:])].append(char_image_full_path)\n",
    "\n",
    "        # random component used to make the selection of which images to use and in which data\n",
    "        # set (training or test) random\n",
    "        random.shuffle(handwritten_uppercase)\n",
    "        random.shuffle(handwritten_lowercase)\n",
    "        random.shuffle(fonts)\n",
    "\n",
    "        # select the calculated number of handwritten characters to be used in the training\n",
    "        # data per class/case combination for both the sets of uppercase and lowercase\n",
    "        # handwritten characters from this class\n",
    "        handwritten_train_images = handwritten_uppercase[:handwritten_train_per_class]\n",
    "        handwritten_train_images.extend(handwritten_lowercase[:handwritten_train_per_class])\n",
    "\n",
    "        # select the calculated number of printed character fonts to be used in the training\n",
    "        # data; all other printed character fonts will be used in the test data\n",
    "        printed_train_images, printed_test_images = [], []\n",
    "        for index, font_num in enumerate(fonts):\n",
    "            if index < printed_train_fonts_per_class:\n",
    "                # all characters using this font will be used in the training data\n",
    "                printed_train_images.extend(printed_by_font[font_num])\n",
    "            else:\n",
    "                # all characters using this font will be used in the test data\n",
    "                printed_test_images.extend(printed_by_font[font_num])\n",
    "\n",
    "        train_images.extend(handwritten_train_images + printed_train_images)\n",
    "        test_images.extend(printed_test_images)\n",
    "\n",
    "    return train_images, test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training/test split has been set constant at 85/15 while training our model, but it is left as a variable parameter. The specified data directory can also be changed, if need be. After trying different multiples of 5% ranging from 50%-100% for the percentage of printed characters to use in the training set, we found that a printed/handwritten split in the training data of 75/25 received the \"best\" results from looking at test accuracy of the model and accuracy on a couple of the generated title pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_files, test_image_files = get_data(\n",
    "    train_ratio=0.85,\n",
    "    printed_train_ratio=0.75,\n",
    "    data_dir=\"data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Handwritten/Printed Data Sources into One Consistent Dataset\n",
    "\n",
    "Since the original formats of the EMNIST (handwritten) data source and the Chars74K (printed) data source are different, we need to convert both data sources into one consistent format. The first difference (handwritten characters being white text on a black background and the printed characters being the reverse of that) was handled outside of this notebook and was applied before zipping up all our data. The next difference (handwritten characters are 28x28 pixels in size whereas printed characters are 128x128 pixels in size) will be solved by following the same process used the produce the EMNIST, handwritten characters. The EMNIST characters originated as 128x128 pixel images in the NIST Special Database 19 database. The process used to convert these 128x128 pixel images to the 28x28 pixel images we see in our dataset (meant to match the format of the original MNIST dataset) is outlined in the [EMNIST paper](https://arxiv.org/pdf/1702.05373v2.pdf) and is shown below.\n",
    "\n",
    "![EMNIST Conversion Process](./Data/emnist_conversion_process.png)\n",
    "\n",
    "We will roughly follow this same process to convert the 128x128 pixel printed images in our dataset to the same format as the 28x28 pixel handwritten images in our dataset. This process can be viewed as one of the layers in our model (the first layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# border padding used when the region of interest is first placed into a square image; caused\n",
    "# a bit of confusion since this is how it is described in the EMNIST paper, but my\n",
    "# interpretation of this description isn't displayed in the image from the EMNIST paper shown\n",
    "# above\n",
    "BORDER_PADDING = 2\n",
    "\n",
    "# maximum pixel value used in the grayscale images; 0.0=black, 255.0=white\n",
    "MAX_PIXEL_VALUE = 255.0\n",
    "\n",
    "# dimensions of the EMNIST data and the target dimensions for the printed data so both data\n",
    "# sources are in the same format\n",
    "TARGET_IMAGE_DIMS = (28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function that transforms images into a target size, following a certain process along\n",
    "the way.\n",
    "\n",
    "Args:\n",
    "    image_files (list): list of full paths of images to transform\n",
    "\n",
    "Returns:\n",
    "    A numpy.ndarray of the transformed images.\n",
    "\"\"\"\n",
    "def transform_data(image_files):\n",
    "\n",
    "    # will store numpy.ndarrays containing the transformed version of each image represented\n",
    "    # by a full path in the input list; will be converted to a numpy.array itself and returned\n",
    "    transformed_data = []\n",
    "\n",
    "    for full_image_path in image_files:\n",
    "\n",
    "        # the transformation process utilizes different libraries including PIL.Image,\n",
    "        # scipy.ndimage, and numpy to be as fast as possible\n",
    "\n",
    "        # load the image using PIL.Image\n",
    "        pixel_array = PIL.Image.open(full_image_path)\n",
    "\n",
    "        # convert to a numpy.ndarray for the next step in the transformation process\n",
    "        cols, rows = pixel_array.size\n",
    "        pixel_matrix = numpy.array(pixel_array, dtype=numpy.float64).reshape((rows, cols))\n",
    "\n",
    "        # if the image is already of the correct size (is handwritten), no need to go through\n",
    "        # the bulk of the transformation process; otherwise (is printed) it does need to go\n",
    "        # through the bulk of the transformation process\n",
    "        if pixel_matrix.shape != TARGET_IMAGE_DIMS:\n",
    "\n",
    "            # apply a Gaussian filter to the image with sigma=1 to soften the edges, using\n",
    "            # scipy.ndimage\n",
    "            pixel_matrix = scipy.ndimage.gaussian_filter(pixel_matrix, sigma=1)\n",
    "\n",
    "            # convert back to a PIL.Image object for the next step in the transformation\n",
    "            # process\n",
    "            pixel_array = PIL.Image.fromarray(pixel_matrix)\n",
    "\n",
    "            # extract the region of interest (remove whitespace surrounding the image)\n",
    "            pixel_array = pixel_array.crop(box=pixel_array.getbbox())\n",
    "\n",
    "            # convert back to a numpy.ndarray for the next step in the transformation process\n",
    "            cols, rows = pixel_array.size\n",
    "            pixel_matrix = numpy.array(pixel_array, dtype=numpy.float64).reshape((rows, cols))\n",
    "\n",
    "            # place and center region of interest into a square image, while preserving aspect\n",
    "            # ratio (add equal amount of whitespace to both sides of the shorter dimension of\n",
    "            # the image until it has square dimensions)\n",
    "            if cols > rows:\n",
    "                if (cols - rows) % 2 == 0:\n",
    "                    row_padding_1 = numpy.zeros(((cols - rows) // 2, cols))\n",
    "                else:\n",
    "                    # if an odd number of rows need to be added to the numpy.ndarray to make\n",
    "                    # it square, add the extra row to the top of the image\n",
    "                    row_padding_1 = numpy.zeros((((cols - rows) // 2) + 1, cols))\n",
    "                row_padding_2 = numpy.zeros(((cols - rows) // 2, cols))\n",
    "                pixel_matrix = numpy.concatenate(\n",
    "                    (row_padding_1, pixel_matrix, row_padding_2),\n",
    "                    axis=0,\n",
    "                )\n",
    "            elif rows > cols:\n",
    "                if (rows - cols) % 2 == 0:\n",
    "                    col_padding_2 = numpy.zeros((rows, (rows - cols) // 2))\n",
    "                else:\n",
    "                    # if an odd number of columns need to be added to the numpy.ndarray to\n",
    "                    # make it square, add the extra column to the right of the image\n",
    "                    col_padding_2 = numpy.zeros((rows, ((rows - cols) // 2) + 1))\n",
    "                col_padding_1 = numpy.zeros((rows, (rows - cols) // 2))\n",
    "                pixel_matrix = numpy.concatenate(\n",
    "                    (col_padding_1, pixel_matrix, col_padding_2),\n",
    "                    axis=1,\n",
    "                )\n",
    "\n",
    "            # add a whitespace border to the square image (keeping it square)\n",
    "            rows, cols = pixel_matrix.shape\n",
    "            row_padding = numpy.zeros((BORDER_PADDING, cols))\n",
    "            pixel_matrix = numpy.concatenate(\n",
    "                (row_padding, pixel_matrix, row_padding),\n",
    "                axis=0,\n",
    "            )\n",
    "            col_padding = numpy.zeros((rows + (2 * BORDER_PADDING), BORDER_PADDING))\n",
    "            pixel_matrix = numpy.concatenate(\n",
    "                (col_padding, pixel_matrix, col_padding),\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            # convert back to a PIL.Image object for the next step in the transformation\n",
    "            # process\n",
    "            pixel_array = PIL.Image.fromarray(pixel_matrix)\n",
    "\n",
    "            # downsample the image to the target dimensions using bi-cubic interpolation\n",
    "            pixel_array = pixel_array.resize(\n",
    "                TARGET_IMAGE_DIMS,\n",
    "                resample=PIL.Image.BICUBIC,\n",
    "            )\n",
    "\n",
    "            # convert back to a numpy.ndarray for the next step in the transformation process\n",
    "            cols, rows = pixel_array.size\n",
    "            pixel_matrix = numpy.array(pixel_array, dtype=numpy.float64).reshape((rows, cols))\n",
    "\n",
    "        # the optional downsampling step in the transformation process can potentially\n",
    "        # increase pixel values over the maximum pixel value, this step reduces these pixel\n",
    "        # values back down to the maximum pixel value; this step also scales the pixel values\n",
    "        # down to a range of [0, 1] for ease of use in the model\n",
    "        _reduce_pixel_values_over_max_and_scale = numpy.vectorize(\n",
    "            lambda pixel_value: min(pixel_value / MAX_PIXEL_VALUE, 1.0)\n",
    "        )\n",
    "        pixel_matrix = _reduce_pixel_values_over_max_and_scale(pixel_matrix)\n",
    "\n",
    "        transformed_data.append(pixel_matrix)\n",
    "\n",
    "    transformed_data = numpy.array(transformed_data)\n",
    "\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function that extracts the labeled classes from the full paths of images.\n",
    "\n",
    "Args:\n",
    "    image_files (list): list of full paths of images to extract class labels from\n",
    "\n",
    "Returns:\n",
    "    A numpy.array of the class labels.\n",
    "\"\"\"\n",
    "def get_labels(image_files):\n",
    "\n",
    "    # will store the class labels encoded in all the full paths of images provided as input\n",
    "    labels = []\n",
    "\n",
    "    for image_file in image_files:\n",
    "        # the class label is the numbered subdirectory (first directory up from the image\n",
    "        # file) in the image's full path\n",
    "        label = int(image_file.split(os.sep)[-2])\n",
    "        labels.append(label)\n",
    "\n",
    "    labels = numpy.array(labels)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_images = transform_data(train_image_files)\n",
    "test_images = transform_data(test_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = get_labels(train_image_files)\n",
    "test_labels = get_labels(test_image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The final model we came up with a 2-layer CNN (64-32) with a 4x4 kernel size and a cross-entropy loss function, trained for 2 epochs. This model was deemed \"best\" from looking a the test accuracy of the model and how the model performed at classifying characters extracted from our generated title pages.\n",
    "\n",
    "We had started with a basic neural network and kept on improving from there. The first big improvement (not directly related with tuning the model) was when we changed our target format from black text on a white background to white text on a black background (easier for the model to deal with since there is more non-text space in the images and the color black is equivalent to a pixel value of 0). The next big improvement was when we shifted from using a basic neural network to a convolutional neural network. A CNN seemed like the better choice for our image-structured data. From there, it's been slight tuning to many different parameters, mainly focusing on the split of printed/handwritten characters in the training set, kernel size, and number of epochs to train for, but there are many parameters in the model that have remain fixed and have yet to be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "56940/56940 [==============================] - 91s 2ms/step - loss: 0.4700 - acc: 0.8690\n",
      "Epoch 2/2\n",
      "56940/56940 [==============================] - 93s 2ms/step - loss: 0.2064 - acc: 0.9382\n",
      "10192/10192 [==============================] - 6s 599us/step\n",
      "Test accuracy: 0.9267072213500785\n"
     ]
    }
   ],
   "source": [
    "model = tensorflow.keras.Sequential([\n",
    "    # adds a dimension for the CNN (this step wasn't necessary to train a basic neural\n",
    "    # network)\n",
    "    tensorflow.keras.layers.Reshape(TARGET_IMAGE_DIMS + (1,), input_shape=TARGET_IMAGE_DIMS),\n",
    "    tensorflow.keras.layers.Conv2D(\n",
    "        64,\n",
    "        kernel_size=4,\n",
    "        activation=tensorflow.nn.relu,\n",
    "    ),\n",
    "    tensorflow.keras.layers.Conv2D(\n",
    "        32,\n",
    "        kernel_size=4,\n",
    "        activation=tensorflow.nn.relu,\n",
    "    ),\n",
    "    tensorflow.keras.layers.Flatten(),\n",
    "    tensorflow.keras.layers.Dense(26, activation=tensorflow.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=2)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment out the line below if you would like to re-train the model or train a new model\n",
    "# to use in our full pipeline\n",
    "\n",
    "#tensorflow.keras.models.save_model(model, \"./Data/model.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
